alg: edr  #htm, dtw, edr, VARIMA, NBEATSModel, TCNModel, TransformerModel, RNNModel, LightGBMModel
hz: 10
test_prop: 0.3
data_cap: 100000 #300
train_models: True
forecast_horizon: 1
time_col: timestamp
eval_metric: rmse #mae, mse, rmse, mape, mase, ope, marre, r2_score, dtw_metric
####################
test_mode: online #batch, online
window_size: 10
scaling: False  #false, minmax, standard


test_indices:
  - 1
  - 2
  - 3

features:
  in:
    - xs
    - ys
    - zs
  pred:
    - xs
    - ys
    - zs

algs_bestfieldsets:
  dtw: [xs, ys, zs]
  edr: [ys, zs, dists]
  htm: [xs, ys, zs, dists]
  lstm: [xs, ys, zs]

algs_types:
  htm: anomaly
  VARIMA: prediction
  NBEATSModel: prediction
  TCNModel: prediction
  TransformerModel: prediction
  RNNModel: prediction
  LightGBMModel: prediction
  # lstm: prediction
  # arima: prediction
  dtw: distance
  edr: distance

colinds_features:
  '0.3': xs
  '0.4': ys
  '0.5': zs
  '0.8': dists_x
  '0.9': dists_y
  '0.10': dists_z
  '0': dists

dirs:
  input: /Users/samheiserman/Desktop/repos/control-print/data/subjects_raw
  data: /Users/samheiserman/Desktop/repos/control-print/data/subjects_preprocessed
  models: /Users/samheiserman/Desktop/repos/control-print/output/models
  results: /Users/samheiserman/Desktop/repos/control-print/output/rank_scores
  scalers: /Users/samheiserman/Desktop/repos/control-print/output/scalers

htm_config:
  features:
    - xs
    - ys
    - zs
    - dists
  models_encoders:
    minmax_percentiles:
      - 1
      - 99
    n: 1000 #700
    n_buckets: 140
    sparsity: 0.02
    timestamp:
      enable: false
      feature: satellite_time
      timeOfDay:
        - 30
        - 1
      weekend: 21
  models_params:
    anomaly:
      period: 1000
    predictor:
      sdrc_alpha: 0.1
    sp:
      boostStrength: 2.0
      columnCount: 2048
      localAreaDensity: 0.04395604395604396
      potentialPct: 0.8
      synPermActiveInc: 0.05  #0.003
      synPermConnected: 0.1  #0.2
      synPermInactiveDec: 0.085  #0.0005
    tm:
      activationThreshold: 13
      cellsPerColumn: 32
      initialPerm: 0.21
      maxSegmentsPerCell: 128
      maxSynapsesPerSegment: 32
      minThreshold: 10
      newSynapseCount: 20
      permanenceConnected: 0.3
      permanenceDec: 0.1
      permanenceInc: 0.1
  models_state:
    model_for_each_feature: false
    save_outputs_accumulated: true
    use_sp: false
    track_iter: 10
    track_tm: false
  timesteps_stop:
    learning: 100000
    running: 110000
    sampling: 30

modnames_grids:
  VARIMA:
    p:
      - 1
      - 3
    d:
      - 0
    q:
      # - 1
      - 3
    # trend: 'c' ['', '', '']
  NBEATSModel:
    # output_chunk_length: int,
    input_chunk_length:
      - 5
      # - 10
    num_stacks:
      - 10
      # - 30
    num_blocks:
      - 1
      # - 2
    num_layers:
      - 1
      # - 2
    layer_widths:
      - 128
      # - 256
    dropout:
      - 0.0
      - 0.2
    # generic_architecture: True
    # expansion_coefficient_dim: 5
    # trend_polynomial_degree: 2
    # activation: "ReLU"  ## ['ReLU','RReLU', 'PReLU', 'Softplus', 'Tanh', 'SELU', 'LeakyReLU',  'Sigmoid']
  TCNModel:
    # output_chunk_length: int,
    input_chunk_length:
      - 5
      - 10
    num_filters:
      - 3
    kernel_size:
      - 3
    dilation_base:
      - 2
    weight_norm:
      - False
    dropout:
      # - 0.2
      - 0.4
    # num_layers: int,
    # dropout_fn,
  TransformerModel:
    # output_chunk_length: int,
    input_chunk_length:
      - 5
      - 10
    d_model:
      - 64
    nhead:
      - 2
      # - 4
    num_encoder_layers:
      - 2
      # - 3
    num_decoder_layers:
      - 2
      # - 3
    dim_feedforward:
      - 512
    dropout:
      - 0.1
      # - 0.2
    activation:
      - "relu"  ## ["relu", "gelu"]
    # custom_encoder: Optional[nn.Module] = None,
    # custom_decoder: Optional[nn.Module] = None,
  RNNModel:
    # input_chunk_length: int,
    input_chunk_length:
      - 5
      # - 10
    training_length:
      - 5
      # - 10
    model: ## ["RNN", "LSTM", "GRU"]
      # - RNN
      - LSTM
    hidden_dim:
      # - 10
      - 25
    n_rnn_layers:
      - 1
      - 2
    dropout:
      # - 0.0
      - 0.2
    # training_length: int = 24,
  LightGBMModel:
    lags:
      # - 1
      - 5
      - 10
    # lags_past_covariates: Union[int, List[int]] = None,
    # lags_future_covariates: Union[Tuple[int, int], List[int]] = None,
    # output_chunk_length: int = 1,
    # quantiles: List[float] = None,
    # random_state: Optional[int] = None,
    # add_encoders: Optional[dict] = None,
      # = {
      # 'cyclic': {'future': ['month']},
      # 'datetime_attribute': {'future': ['hour', 'dayofweek']},
      # 'position': {'past': ['absolute'], 'future': ['relative']},
      # 'custom': {'past': [lambda idx: (idx.year - 1950) / 50]},
      # 'transformer': Scaler()
      # }
    # likelihood: str = None,
    #   = ['quantile', 'poisson']

# lstm_config:
#   shuffle: true # true/false
#   stateful: true # true/false
#   batch_size: 1 #1/1000
#   reset_states: true
#   n_epochs: 100
#   n_layers: 2
#   n_units: 50 #1-100
#   activation: relu
#   optimizer: adam
#   loss: mean_squared_error #mse
# features:
#   - xs
#   - ys
#   - zs
#   # - dists